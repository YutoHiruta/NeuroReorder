import numpy as np
import gym
import gym.spaces

from rulemodel import *

#なんかそういう
#環境あるんですか？
#
#　　　／￣￣￣＼
#　　／　　　　　＼
#　 /／|　　　　　 ヽ
#　//　ヽ(、　　　　|
#`/ｲヽ ／⌒＼　　　 |
#　|･)　(･＞ ヽ　　 |
#　|/　　　　 |　　 |
#　(_へ)ヽ　　|／|　|
#　 ﾚ亠ー､)　　 ノ　|
#　 ヽ二／　／　 ヽノ_
#／￣(wwww／　　/ |　 ＼
#　　|￣|　　 ／　|
#　　|／ヽ　 /⌒＼|　 /
#⌒⌒ヽ ﾟ|＼∧　 ／⌒二)
#∩∩ | ﾟ|　 ∧／　　ヽ


#
# -- 強化学習の環境を定義するクラス --
#
class rulemodel_env(gym.core.Env):
    #====================================
    #=            クラス初期化            =
    #====================================
    def __init__(self,rulelist,max_steps):
        # パラメータを設定
        self.rulelist = rulelist
        self.max_steps = max_steps
        self.max_stay = 200

        
        self.steps = 0
        self.stay_num = 0


        #現在実装済みのアクションの数 (STAY、MOVEのふたつ)
        self.implemented_action_num = 2
        
        # ルーリストの大きさとビット長（環境）を決定
        self.rulelist_len = len(rulelist)
        self.bit_string_len = len(rulelist[0]) + 1


        # 行動空間を設定(0-STAY 1-MOVE)
        #self.action_space = gym.spaces.Discrete(2)

        #各次元の上限値
        # 1 -> アクション数
        # 2 -> ルール数 (アクション適用先のルール位置 , 一部アクションでは未使用)
        # 3 -> ルール数 (アクション適用先のルール位置2 (例：MOVE先))
        high = np.array(
            [
                self.implemented_action_num - 1,
                self.rulelist_len,
                self.rulelist_len,
            ],
            dtype=np.int16,)        
        
        self.action_space = gym.spaces.Box(
            low=0,
            high=high,
            shape=(3,),
            dtype=np.int16)
        
        
        # 状態空間を設定(リスト長 x ビット長+1(評価型を先頭に付与))
        # bit値 0->0  1->1  *->2
        # 評価型 D->0  P->1
        self.observation_space = gym.spaces.Box(
            low=0,
            high=3,
            shape=(self.rulelist_len,self.bit_string_len),
            dtype=np.int16)

        
    #====================================
    #=             環境初期化             =
    #====================================
    def _reset(self):

        self.step_num = 0
        self.stay_num = 0

        
        return _transform_rulelist_to_state()
        
        
    #====================================
    #=           1ステップの処理           =
    #====================================
    def _step(self,action):


        #変数初期化
        reward = 0

        # action [i,j,k]
        
        # アクション STAY
        # -> 何もせず待機（ただし報酬-1）
        if action[0] == 0:
            reward -= 1
            self.stay_num += 1
        # アクション MOVE
        # -> ルールjをルールkの位置へ移動（失敗した(従属関係を考慮しない移動になった)場合は報酬-10して状態維持
        elif action[0] == 1:

            success = False
            
            # 同値はエラーとして罰を与える
            if action[1] == action[2]:
                reward -= 10

            elif action[1] >= action[2]:
                success = self.rulelist.action_move(action[1].action[2])
            

            

            

        
        return

    
    #====================================
    #= ルールリストを状態の構造へ変換する関数 =
    #====================================
    def _transform_rulelist_to_state(self):
        state = []            
        
        for i in range(len(self.rulelist)):
            element = []
            #先頭に評価型を数値として追加
            if self.rulelist[i].evaluate == "Accept":
                element.append(1)
            else:
                element.append(0)

            #残りの要素を追加
            element.extend(self.rulelist[i].bit_string_num)

            state.append(element)

        return state


    def render(self):
        pass
